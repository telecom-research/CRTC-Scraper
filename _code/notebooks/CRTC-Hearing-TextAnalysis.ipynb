{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRTC Hearing Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to illustrate the method of text analysis using a corpus created from digital content published by the CRTC. This is the second part in a two-part process, the first of which is a description of the code that 'scraped' the CRTC webpage to create the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below imports the modules that are required to process the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing code modules\n",
    "import json\n",
    "import ijson\n",
    "from ijson import items\n",
    "\n",
    "import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "import nltk\n",
    "import nltk.collocations\n",
    "import collections\n",
    "import statistics\n",
    "from nltk.metrics.spearman import *\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# This is a function for reading the contents of files\n",
    "def read_file(filename):\n",
    "    \"Read the contents of FILENAME and return as a string.\"\n",
    "    infile = codecs.open(filename, 'r', 'utf-8')\n",
    "    contents = infile.read()\n",
    "    infile.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the File\n",
    "This code loads and then reads the necessary files: the `json` file with all the hearing text, and a `txt` file with a list of stopwords, taken from here: http://www.lextek.com/manuals/onix/stopwords2.html. I've also added a few custom words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the JSON file\n",
    "filename = \"../scrapy/hearing_result6.json\"\n",
    "\n",
    "# loading the stopwords file\n",
    "stopwords = read_file('cornellStopWords.txt')\n",
    "customStopwords = stopwords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the file and assigns the keys and values to a Python dictionary structure\n",
    "with open(filename, 'r') as f:\n",
    "    objects = ijson.items(f, 'item')\n",
    "    file = list(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of error checking here to confirm the number of records in the file. We should have 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# checks to see how many records we have\n",
    "print(len(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the number in the code below will print a different record from the file. Please remember that in coding, numbered lists begin at `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# commenting this out to make the github notebook more readable.\n",
    "# prints all content in a single record. Changing the number shows a different record\n",
    "file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bit more error checking to confirm the record titles and their urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterates through each record in the file\n",
    "for row in file:\n",
    "    # prints the title of each record and its url\n",
    "    print(row['title'], \":\", row['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a bit more processing to make the text more readable. It's printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appends all of the text items to a single string object (rather than a list)\n",
    "joined_text = []\n",
    "for row in file:\n",
    "    joined_text.append(' '.join(row['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shows the text. Changing the number displays a different record...\n",
    "# ...changing/removing the second number limits/expands the text shown.\n",
    "print(joined_text[5][:750])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Processing\n",
    "This is the begining of the first processing for the text analysis. Here we will split all the words apart, make them all lowercase, and remove the punctuation, numbers, and words on the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# splits the text string in each record into a list of separate words\n",
    "token_joined = []\n",
    "for words in joined_text:\n",
    "    # splits the text into a list of words\n",
    "    text = words.split()\n",
    "    # makes all words lowercase\n",
    "    clean = [w.lower() for w in text if w.isalpha()]\n",
    "    # applies stopword removal\n",
    "    text = [w for w in clean if w not in customStopwords]\n",
    "    token_joined.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a word of interest is `guarantee`, here is a list of how many times that word (and its variations) appear in each record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for title,word in zip(file,token_joined):\n",
    "   # print(title['title'],\"guarantee:\", word.count('guarantee'), \"guarantees:\", \\\n",
    "       #   word.count('guarantees'), \"guaranteed:\", word.count('guaranteed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript, Hearing April 11, 2016  service: 210 services: 103\n",
      "Transcript, Hearing April 12, 2016  service: 166 services: 74\n",
      "Transcript, Hearing April 13, 2016  service: 98 services: 49\n",
      "Transcript, Hearing April 14, 2016  service: 98 services: 31\n",
      "Transcript, Hearing April 15, 2016  service: 76 services: 31\n",
      "Transcript, Hearing April 18, 2016  service: 215 services: 79\n",
      "Transcript, Hearing April 19, 2016  service: 90 services: 46\n",
      "Transcript, Hearing April 20, 2016  service: 118 services: 60\n",
      "Transcript, Hearing April 21, 2016  service: 137 services: 52\n",
      "Transcript, Hearing April 22, 2016  service: 30 services: 30\n",
      "Transcript, Hearing April 25, 2016  service: 208 services: 60\n",
      "Transcript, Hearing April 26, 2016  service: 157 services: 60\n",
      "Transcript, Hearing April 27, 2016  service: 62 services: 28\n",
      "Transcript, Hearing April 28, 2016  service: 34 services: 20\n"
     ]
    }
   ],
   "source": [
    "for title,word in zip(file,token_joined):\n",
    "    print(title['title'],\"service:\", word.count('service'),\"services:\", word.count('services'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "It looks like record number 5 has the most occurences of the word `guarantee`. The code below isolates the record and creates a concordance based on the selected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splits the text from the record into a list of individual words\n",
    "words = joined_text[0].split()\n",
    "#assigns NLTK functionality to the text\n",
    "text = nltk.Text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 79 matches:\n",
      "tion like YouTube with captioning; services supplied online using sign languag\n",
      " sign language, for example, relay services like video relay interpreting and \n",
      "want to add that telecommunication services should be recognized as a basic se\n",
      " catch up. 7012 Broadband internet services must be defined as a basic service\n",
      "indication display of what type of services would be offered for the deaf comm\n",
      "’m unaware of any direct frontline services provided in sign language. But I a\n",
      "us about getting rid of any of the services we have currently. 7072 Now, maybe\n",
      "one who works at telecommunication services to understand deaf culture and per\n",
      "d, so that we can provide the best services for our community going forward. 7\n",
      "oned earlier that the packages and services available to the deaf community we\n",
      "herwise, they’ll be unaware of any services being offered. 7100 MR. ROOTS: Jim\n",
      " enhance our understanding of what services are available and see which produc\n",
      "ailable and see which products and services will meet our needs. 7108 And that\n",
      "ordable and reliable communication services are increasingly essential, for th\n",
      "ibility of media and communication services by 2020. 7177 The Access 2020 Coal\n",
      "eliable, fixed and mobile internet services that are essential for participati\n",
      "excluding broadband from the basic services framework. 7187 We recognize that \n",
      "t the affordability and quality of services that are available to them. 7191 E\n",
      "cess to fixed and mobile broadband services and the need for deploying the wid\n",
      "ents in improving accessibility of services they deliver or responding to the \n",
      "es can access basic communications services that are essential to their abilit\n",
      "ccess and use basic communications services they need. 7206 Even though incumb\n",
      "her information and communications services and applications. 7207 We submit t\n",
      "cerns about affordability of basic services for Canadians with low incomes, as\n",
      "to and use of basic communications services by Canadians with severe or very s\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# prints a concordance output for the selected word (shown in green)\n",
    "print(text.concordance('services', lines=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a new file that can be written by the print queue\n",
    "fileconcord = codecs.open('April11_service_concord.txt', 'w', 'utf-8')\n",
    "#makes a copy of the empty print queue, so that we can return to it at the end of the function\n",
    "tmpout = sys.stdout\n",
    "#stores the text in the print queue\n",
    "sys.stdout = fileconcord\n",
    "#generates and prints the concordance, the number pertains to the total number of bytes per line\n",
    "text.concordance(\"service\", 79, sys.maxsize)\n",
    "#closes the file\n",
    "fileconcord.close()\n",
    "#returns the print queue to an empty state\n",
    "sys.stdout = tmpout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what the text looks like after the initial processing, without punctuation, numbers, or stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shows the text list for a given record. Changing the first number displays a... \n",
    "# ...different record, changing/removing the second number limits/expands the text shown\n",
    "print(token_joined[5][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Some more preparation for the text processing. The code below works on the all of the records, creating one master list of words which is then lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates a variable for the lemmatizing function\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# lemmatizes all of the verbs\n",
    "\n",
    "lemm = []\n",
    "for record in token_joined:\n",
    "        for word in record:\n",
    "            lemm.append(wnl.lemmatize(word, 'v'))\n",
    "'''\n",
    "lemm = []\n",
    "for word in token_joined[13]:\n",
    "        lemm.append(wnl.lemmatize(word, 'v'))\n",
    "'''\n",
    "\n",
    "# lemmatizes all of the nouns \n",
    "lems = []\n",
    "for word in lemm:\n",
    "    lems.append(wnl.lemmatize(word, 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are checking to make sure the lemmatizer has worked. Now the word `guarantee` only appears in one form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just making sure the lemmatizer has worked\n",
    "#print(\"guarantee:\", lems.count('guarantee'), \"guarantees:\", \\\n",
    "         # lems.count('guarantees'), \"guaranteed:\", lems.count('guaranteed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service: 2435 0\n"
     ]
    }
   ],
   "source": [
    "print(\"service:\", lems.count('service'), lems.count('services'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "Here is a count of the number of words in each record. While this data isn't terribly useful 'as is', we can make a few assumptions about the text here. Notably that some of the hearings were much longer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript, Hearing April 11, 2016  : 16664 words\n",
      "Transcript, Hearing April 12, 2016  : 12891 words\n",
      "Transcript, Hearing April 13, 2016  : 8423 words\n",
      "Transcript, Hearing April 14, 2016  : 8319 words\n",
      "Transcript, Hearing April 15, 2016  : 4840 words\n",
      "Transcript, Hearing April 18, 2016  : 13523 words\n",
      "Transcript, Hearing April 19, 2016  : 10184 words\n",
      "Transcript, Hearing April 20, 2016  : 8541 words\n",
      "Transcript, Hearing April 21, 2016  : 12865 words\n",
      "Transcript, Hearing April 22, 2016  : 3400 words\n",
      "Transcript, Hearing April 25, 2016  : 14791 words\n",
      "Transcript, Hearing April 26, 2016  : 11804 words\n",
      "Transcript, Hearing April 27, 2016  : 7454 words\n",
      "Transcript, Hearing April 28, 2016  : 6803 words\n"
     ]
    }
   ],
   "source": [
    "# counting the number of words in each record \n",
    "for name, each in zip(file,token_joined):\n",
    "    print(name['title'], \":\",len(each), \"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will count the five most common words in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docfreq = []\n",
    "for words in token_joined:\n",
    "    docfreq.append(nltk.FreqDist(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, words in zip(file_obj, docfreq):\n",
    "    print(name['title'], \":\", words.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 10 most common word pairs in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service provider; market force; digital literacy; basic service; data\n",
      "cap; eastern ontario; fix wireless; private sector; low income; rural\n",
      "remote\n"
     ]
    }
   ],
   "source": [
    "# prints the 10 most common bigrams\n",
    "colText = nltk.Text(lems)\n",
    "colText.collocations(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error checking to make sure the code is processing the text properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hear', 'april'),\n",
       " ('april', 'quebec'),\n",
       " ('quebec', 'april'),\n",
       " ('april', 'copyright'),\n",
       " ('copyright', 'reserve')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a list of bigrams (ngrams of 2), printing the first 5\n",
    "colBigrams = list(nltk.ngrams(colText, 2)) \n",
    "colBigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More error checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# error checking. There should be one less bigram than total words\n",
    "print(\"Number of words:\", len(lems))\n",
    "print(\"Number of bigrams:\", len(colBigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a frequency plot showing the occurence of the 25 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency plot with stopwords removed\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "fd = nltk.FreqDist(colText)\n",
    "fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "Here we are preparing the text to search for bigrams containing the word `guarantee`. This code searches for words appearing before and after `guarantee` with a window size of two words on either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loads bigram code from NLTK\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# bigrams with a window size of 2 words\n",
    "finder = BigramCollocationFinder.from_words(lems, window_size = 2)\n",
    "# ngrams with 'word of interest' as a member\n",
    "word_filter = lambda *w: 'service' not in w\n",
    "# only bigrams that contain the 'word of interest'\n",
    "finder.apply_ngram_filter(word_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter results based on statistical test\n",
    "\n",
    "# calulates the raw frequency as an actual number and percentage of total words\n",
    "act = finder.ngram_fd.items()\n",
    "raw = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "# log-likelihood ratio\n",
    "log = finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research shows that this is the most reliable statistical test for unreliable data. \n",
    "\n",
    "**Log-Likelihood Ratio**\n",
    "\n",
    "The Log-likelihood ratio calculates the size and significance between the observed and expected frequencies of bigrams and assigns a score based on the result, taking into account the overall size of the corpus. The larger the difference between the observed and expected, the higher the score, and the more statistically significant the collocate is.\n",
    "The Log-likelihood ratio is my preferred test for collocates because it does not rely on a normal distribution, and for this reason, it can account for sparse or low frequency bigrams. It does not over-represent low frequency bigrams with inflated scores, as the test is only reporting how much more likely it is that the frequencies are different than they are the same. The drawback to the Log-likelihood ratio is that it cannot be used to compare scores across corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note here that **words will appear twice** in the following list. As the ngrams can appear both before and after the word, care must be taken to identify duplicate occurences in the list below and then combine the totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collocate                        Log-Likelihood\n",
      "-------------------------------  ----------------\n",
      "('digital', 'literacy')          450.282\n",
      "('literacy', 'skill')            39.179\n",
      "('literacy', 'train')            12.498\n",
      "('iterate', 'literacy')          11.203\n",
      "('literacy', 'critically')       11.203\n",
      "('literacy', 'nowadays')         11.203\n",
      "('literacy', 'telecommunities')  11.203\n",
      "('literacy', 'mean')             9.784\n",
      "('literacy', 'conceive')         7.399\n",
      "('literacy', 'throw')            7.399\n",
      "('literacy', 'express')          6.726\n",
      "('literacy', 'factor')           6.726\n",
      "('literacy', 'free')             6.726\n",
      "('literacy', 'poverty')          6.228\n",
      "('literacy', 'primary')          5.833\n",
      "('literacy', 'actual')           5.505\n",
      "('literacy', 'reflect')          5.505\n",
      "('literacy', 'option')           5.226\n",
      "('literacy', 'potential')        4.983\n",
      "('literacy', 'benefit')          4.767\n",
      "('literacy', 'enhance')          4.767\n",
      "('literacy', 'hand')             4.767\n",
      "('literacy', 'responsibility')   4.767\n",
      "('literacy', 'involve')          4.574\n",
      "('literacy', 'extent')           4.400\n",
      "('literacy', 'job')              4.400\n",
      "('literacy', 'couple')           3.832\n",
      "('literacy', 'essential')        3.832\n",
      "('literacy', 'guess')            3.221\n",
      "('literacy', 'comment')          2.982\n",
      "('literacy', 'ensure')           2.982\n",
      "('literacy', 'increase')         2.982\n",
      "('literacy', 'country')          2.839\n",
      "('literacy', 'move')             2.773\n",
      "('literacy', 'bite')             2.647\n",
      "('bite', 'literacy')             2.647\n",
      "('literacy', 'time')             2.530\n",
      "('literacy', 'build')            2.370\n",
      "('literacy', 'kind')             1.826\n",
      "('literacy', 'part')             1.792\n",
      "('literacy', 'work')             1.164\n",
      "('fund', 'literacy')             1.144\n",
      "('literacy', 'issue')            0.930\n",
      "('literacy', 'digital')          0.854\n",
      "('literacy', 'make')             0.797\n",
      "('literacy', 'people')           0.441\n"
     ]
    }
   ],
   "source": [
    "# prints list of results. \n",
    "print(tabulate(log, headers = [\"Collocate\", \"Log-Likelihood\"], floatfmt=\".3f\", \\\n",
    "               numalign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collocate                       Actual\n",
      "------------------------------  --------\n",
      "('literacy', 'responsibility')  1\n",
      "('ask', 'literacy')             1\n",
      "('literacy', 'gap')             1\n",
      "('bridge', 'literacy')          1\n",
      "('literacy', 'support')         1\n",
      "('literacy', 'skill')           1\n",
      "('literacy', 'objective')       1\n",
      "('literacy', 'piece')           3\n",
      "('literacy', 'largely')         1\n",
      "('literacy', 'problem')         2\n",
      "('literacy', 'digital')         1\n",
      "('thought', 'literacy')         1\n",
      "('literacy', 'huge')            1\n",
      "('digital', 'literacy')         10\n"
     ]
    }
   ],
   "source": [
    "# prints list of results. \n",
    "print(tabulate(act, headers = [\"Collocate\", \"Actual\"], floatfmt=\".3f\", \\\n",
    "               numalign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('digital-literacy_collocate_Act.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of words appearing twice. Below are both instances of the ngram 'quality'. The first instance appears before 'guarantee' and the second occurs after."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Collocate                     Log-Likelihood\n",
    "----------------------------  ----------------\n",
    "('quality', 'guarantee')      76.826\n",
    "('guarantee', 'quality')      3.955\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit more processing to clean up the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "############### sorts list of log-likelihood scores ##############\n",
    "##################################################################\n",
    "\n",
    "# group bigrams by first and second word in bigram                                        \n",
    "prefix_keys = collections.defaultdict(list)\n",
    "for key, l in log:\n",
    "    # first word\n",
    "    prefix_keys[key[0]].append((key[1], l))\n",
    "    # second word\n",
    "    prefix_keys[key[1]].append((key[0], l))\n",
    "    \n",
    "# sort bigrams by strongest association                                  \n",
    "for key in prefix_keys:\n",
    "    prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "    # prints top 80 results\n",
    "logkeys = prefix_keys['service'][:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list showing **only** the collocates for the word `guarantee`. Again, watch for duplicate words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collocate          Log-Likelihood\n",
      "-----------------  ----------------\n",
      "provider           1300.460\n",
      "basic              1084.240\n",
      "telecommunication  328.872\n",
      "internet           267.560\n",
      "quality            221.506\n",
      "objective          180.661\n",
      "provide            164.326\n",
      "broadband          141.885\n",
      "telephone          135.467\n",
      "universal          107.138\n",
      "social             101.779\n",
      "minimum            93.220\n",
      "wireless           90.850\n",
      "communication      90.108\n",
      "level              87.269\n",
      "essential          74.443\n",
      "telecom            69.281\n",
      "relay              58.368\n",
      "quality            55.463\n",
      "level              47.471\n",
      "improve            41.829\n",
      "offer              41.147\n",
      "include            39.013\n",
      "rural              37.936\n",
      "provide            36.162\n",
      "voice              33.907\n",
      "deliver            32.591\n",
      "meg                27.341\n",
      "agency             27.287\n",
      "obligation         24.594\n",
      "high               23.735\n",
      "discretionary      22.993\n",
      "offer              22.866\n",
      "emergency          22.337\n",
      "satellite          21.426\n",
      "cell               21.202\n",
      "product            20.738\n",
      "cellular           20.349\n",
      "extend             20.070\n",
      "improvement        19.911\n",
      "dsl                18.344\n",
      "deliver            17.959\n",
      "delivery           17.797\n",
      "overbuilt          16.223\n",
      "telecomm           16.223\n",
      "migrate            16.122\n",
      "fund               15.920\n",
      "canadian           14.736\n",
      "package            14.663\n",
      "voip               14.450\n",
      "definition         13.657\n",
      "comparable         13.302\n",
      "hear               12.441\n",
      "rep                12.439\n",
      "issue              12.342\n",
      "area               12.166\n",
      "purchase           11.911\n",
      "luxury             11.724\n",
      "outage             11.724\n",
      "application        11.496\n",
      "web                11.414\n",
      "provision          11.207\n",
      "bundle             11.192\n",
      "obtain             10.892\n",
      "work               10.836\n",
      "satisfactory       10.747\n",
      "type               10.657\n",
      "thing              10.461\n",
      "fibre              10.412\n",
      "mko                10.366\n",
      "directly           10.173\n",
      "wireline           10.120\n",
      "subsidy            10.043\n",
      "canadian           9.799\n",
      "delivery           9.625\n",
      "wrap               9.597\n",
      "point              9.336\n",
      "enhance            9.282\n",
      "sort               9.225\n",
      "extend             9.139\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate(logkeys, headers = [\"Collocate\", \"Log-Likelihood\"], floatfmt=\".3f\", \\\n",
    "               numalign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('service_collocate_Log.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(logkeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# working on a regex to split text by speaker\n",
    "diced = []\n",
    "for words in joined_text:\n",
    "    diced.append(re.split('(\\d+(\\s)\\w+[A-Z](\\s|.\\s)\\w+[A-Z]:\\s)', words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(diced[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_names = []\n",
    "for words in joined_text:\n",
    "    init_names.append(set(re.findall('[A-Z]{3,}', words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(init_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('initialNames.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(init_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
